{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71a5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c80041c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a414f6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a526a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected 7 variables for PCA\n",
    "selected_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction']\n",
    "df_selected = df[selected_columns]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f475f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratios: [0.29545516 0.17228178 0.14522548]\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# Display the explained variance ratios\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratios:\", explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8a846",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "### 1. **0.29545516 (29.55%)**:\n",
    "   - The **first principal component (PC1)** explains **29.55%** of the total variance in the dataset. This component captures the largest amount of variance compared to the other components, and therefore is the most significant in terms of describing the dataset's variability.\n",
    "\n",
    "### 2. **0.17228178 (17.23%)**:\n",
    "   - The **second principal component (PC2)** explains **17.23%** of the total variance. While it contributes less than PC1, it still captures a substantial portion of the dataset's variance.\n",
    "\n",
    "### 3. **0.14522548 (14.52%)**:\n",
    "   - The **third principal component (PC3)** explains **14.52%** of the total variance. This component captures even less variance, but together with PC1 and PC2, they cover a large portion of the variability.\n",
    "\n",
    "### Key Insights:\n",
    "- **Total explained variance** by the first three principal components is:\n",
    "  \\[\n",
    "  29.55\\% + 17.23\\% + 14.52\\% = 61.30\\%\n",
    "  \\]\n",
    "  - So, the first three components together explain **61.30%** of the total variance in the dataset.\n",
    "  \n",
    "- PCA is often used for **dimensionality reduction**, and with these three components, you could potentially reduce the dimensionality of the data (i.e., reduce the number of features) while retaining much of the variance or information in the original data.\n",
    "  \n",
    "- Since **61.30%** of the variance is captured by these three components, they provide a simplified representation of the dataset with fewer features but with a significant amount of the original structure and relationships preserved.\n",
    "\n",
    "If a higher percentage of variance retention is needed (e.g., 80-90%), you might consider including additional principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd410eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2       PC3\n",
      "0  0.708054  0.631703  0.145194\n",
      "1 -0.951638 -0.979558 -1.014125\n",
      "2 -0.686428  1.608650  2.271100\n",
      "3 -0.776792 -0.903345 -0.841062\n",
      "4  2.719940 -2.809888  2.196097\n"
     ]
    }
   ],
   "source": [
    "# Output the transformed data\n",
    "print(pca_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d51db",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "### Breakdown of each column:\n",
    "- **PC1**: The values in this column represent the projections of the data onto the first principal component. Since PC1 captures the largest amount of variance (29.55%), these values are the most significant in terms of describing the variability of the data.\n",
    "  \n",
    "- **PC2**: This column contains the projections of the data onto the second principal component, which captures 17.23% of the variance. These values describe the second most significant variation in the data.\n",
    "  \n",
    "- **PC3**: The values in this column represent the projections onto the third principal component, capturing 14.52% of the variance.\n",
    "\n",
    "### Sample Data Interpretation:\n",
    "- **Row 0**: [0.708054, 0.631703, 0.145194]\n",
    "  - This data point has moderately positive values across all three principal components, suggesting it is somewhat aligned with the directions captured by these components.\n",
    "  \n",
    "- **Row 1**: [-0.951638, -0.979558, -1.014125]\n",
    "  - This point has large negative values across all components, indicating it is far from the origin in the transformed space and negatively aligned with the directions of PC1, PC2, and PC3.\n",
    "  \n",
    "- **Row 2**: [-0.686428, 1.608650, 2.271100]\n",
    "  - The point is negatively aligned with PC1 but has strong positive projections onto PC2 and PC3, meaning it exhibits variance captured primarily by the second and third principal components.\n",
    "  \n",
    "- **Row 3**: [-0.776792, -0.903345, -0.841062]\n",
    "  - This point has negative values for all components, meaning it is aligned negatively with the variability captured by PC1, PC2, and PC3.\n",
    "  \n",
    "- **Row 4**: [2.719940, -2.809888, 2.196097]\n",
    "  - This point has a strong positive projection onto PC1 and PC3, but a strong negative projection onto PC2, indicating a unique alignment with the variance captured by these components.\n",
    "\n",
    "### Key Insights:\n",
    "- **Dimensionality Reduction**: The dataset has been reduced from its original feature space to three principal components, which summarize the majority of the variance in the dataset (61.30%). These transformed features can now be used for further analysis, such as clustering, regression, or visualization, with reduced complexity.\n",
    "  \n",
    "- **Interpretation**: The values in the transformed space (PC1, PC2, and PC3) no longer directly correspond to the original features but instead represent new axes that capture the most significant directions of variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21643136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principle Component Analysis\n",
    "\n",
    "#Import reuired packages\n",
    "#load dataset\n",
    "#define required functions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "### THIS PLOTING IS BIT TRICKY SO BETTER AVOID IT \n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"G:\\storage 4\\drive_2\\00_vit vellore\\0_teaching assignments\\multivariate data analysis\\lab\\data sets\\autos.csv\")\n",
    "df\n",
    "\n",
    "#We've selected four features that cover a range of properties. Each of these features also has a high MI score with the target, price. We'll standardize the data since #these features aren't naturally on the same scale.\n",
    "\n",
    "features = [\"highway_mpg\", \"engine_size\", \"horsepower\", \"curb_weight\"]\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('price')\n",
    "X = X.loc[:, features]\n",
    "\n",
    "# Standardize\n",
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "#Now we can fit scikit-learn's PCA estimator and create the principal components. You can see here the first few rows of the transformed dataset.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create principal components\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "X_pca.head()\n",
    "\n",
    "\n",
    "#After fitting, the PCA instance contains the loadings in its components_ attribute. (Terminology for PCA is inconsistent, unfortunately. We're following the convention #that calls the transformed columns in X_pca the components, which otherwise don't have a name.) We'll wrap the loadings up in a dataframe.\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # transpose the matrix of loadings\n",
    "    columns=component_names,  # so the columns are the principal components\n",
    "    index=X.columns,  # and the rows are the original features\n",
    ")\n",
    "loadings\n",
    "\n",
    "#Recall that the signs and magnitudes of a component's loadings tell us what kind of variation it's captured. The first component (PC1) shows a contrast between large, #powerful vehicles with poor gas milage, and smaller, more economical vehicles with good gas milage. We might call this the \"Luxury/Economy\" axis. The next figure shows #that our four chosen features mostly vary along the Luxury/Economy axis.\n",
    "\n",
    "# Look at explained variance\n",
    "plot_variance(pca);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
